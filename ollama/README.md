# Ollama (Non-Agentic LLM Server)

This folder documents how to run a local LLM server using [Ollama](https://ollama.com/). The server responds to prompts sent from the Jarvis system.

## Setup

1. **Install Ollama**

   Download from: https://ollama.com/download

2. **Pull the Model**

   For example, to use LLaMA 3:
   ```bash
   ollama pull llama3
